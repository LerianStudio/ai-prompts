---
description: Define focused test coverage linked directly to acceptance criteria
globs: 
alwaysApply: false
---
# Rule: Creating Feature Test Strategy

## ðŸ§  Enhanced Analysis Tools - USE THESE!

**CRITICAL:** Leverage these tools throughout the test strategy creation process:

### ðŸ§© Memory MCP Integration
- **Retrieve test patterns:** Use `mcp__lerian-memory__memory_read` with `operation="search"` for existing test strategies and patterns
- **Find similar tests:** Use `mcp__lerian-memory__memory_read` with `operation="find_similar"` for comparable feature test approaches
- **Store test decisions:** Use `mcp__lerian-memory__memory_create` with `operation="store_decision"` for test strategy rationale
- **Tags to use:** `["test-strategy", "feature-testing", "acceptance-criteria", "feature-name"]`

### ðŸ”„ Sequential Thinking MCP
- **Use for:** Test coverage analysis, risk assessment, test prioritization
- **Pattern:** Acceptance criteria â†’ test scenarios â†’ coverage gaps â†’ risk mitigation
- **Benefit:** Ensures comprehensive test coverage directly linked to feature requirements

### ðŸ§  Zen MCP Tools (TEST INTELLIGENCE)
**Use Zen for test strategy optimization:**
- **`mcp__zen__codereview`** - Identify testable code patterns
  ```bash
  mcp__zen__codereview
    files=["feature/implementation/files"]
    prompt="Identify critical paths and edge cases that need testing for [feature]"
    model="flash"
    review_type="quick"
    focus_on="testability and edge cases"
  ```
- **`mcp__zen__thinkdeep`** - Explore test scenarios and edge cases
  ```bash
  mcp__zen__thinkdeep
    prompt="What are non-obvious test scenarios for [feature] considering user behavior and system integration?"
    model="pro"
    thinking_mode="medium"
    focus_areas=["edge_cases", "error_scenarios", "performance_boundaries"]
  ```

### ðŸš€ Task Tool (TEST PATTERN DISCOVERY)
**Use Task for finding test examples:**
```bash
Task(
  description="Find test patterns",
  prompt="Search for test strategies and test code for similar features, including unit tests, integration tests, and E2E scenarios"
)
```
**Benefits**: Reuse test patterns, understand coverage expectations, learn from test automation approaches

## Goal

To guide an AI assistant in creating a focused test strategy that directly validates acceptance criteria from the feature brief, with complexity-appropriate depth and pattern reuse.

## Process

1. **Extract Acceptance Criteria:** Load feature brief and map all acceptance criteria
   - ðŸ§© **Memory Action:** `memory_search` for feature brief and acceptance criteria
   - **[ADAPT]:** Scale test depth based on feature complexity

2. **Analyze Technical Risks:** Review technical approach for testing implications
   - ðŸ§© **Memory Action:** `memory_read` with `operation="find_similar"` for similar test patterns
   - **[OPTIMIZE]:** Identify reusable test patterns and frameworks

3. **Define Test Coverage:** Map tests directly to acceptance criteria
   - ðŸ”„ **Sequential Thinking:** Each criterion â†’ test scenarios â†’ validation approach
   - **[FLEX]:** Adjust coverage depth based on risk assessment

4. **User Interaction:** Present test priorities for feedback
   - **Simple features:** "I'll focus on [X] key test scenarios. Any specific concerns?"
   - **Complex features:** "Here's my risk-based test prioritization. Please review."
   - **WAIT for user input**

5. **Generate Test Plan:** Create comprehensive test strategy
   - **[OPTIMIZE]:** Include references to similar test implementations
   - ðŸ§© **Memory Action:** `memory_store_decision` for test approach rationale

## Focused Test Strategy Development

Now let's develop a targeted test strategy for your feature. Since this is an incremental addition, we'll focus on efficient, risk-based testing.

### 1. Feature-Specific Test Coverage

#### Unit Tests for New Components
**New Code Requiring Unit Tests**:
- [New functions/methods identified from technical approach]
- [Modified existing functions]
- [New utility/helper functions]

**Question**: Are you adding any complex algorithms or business logic that would benefit from property-based testing?

#### Integration Tests for Touch Points
**Integration Scenarios**:
- [New feature + existing system interactions]
- [Modified API endpoints]
- [Database schema changes impact]

**Key Consideration**: How does this feature interact with existing features? List the main touch points:
1. _______________
2. _______________
3. _______________

#### Targeted E2E Tests
**New User Journeys**:
- [Primary feature workflow]
- [Feature interaction with existing flows]
- [Error scenarios specific to feature]

**Question**: Should we add this to existing E2E tests or create new isolated scenarios?

### 2. Regression Prevention Strategy

#### Impact Analysis
Based on the feature changes, these areas need regression testing:
- [Existing features that share code]
- [Dependent systems]
- [Common user workflows affected]

**Question**: Do you have existing regression tests that cover these areas, or do we need new ones?

#### Contract Testing Updates
**API Changes**:
- New endpoints: [List]
- Modified endpoints: [List]
- Deprecated endpoints: [List]

**Question**: Are there any breaking changes that require version management?

### 3. Feature-Specific Test Types

#### Performance Impact Testing
**Performance Considerations**:
- Additional database queries: [Count/complexity]
- New background jobs: [Description]
- Increased payload sizes: [Estimates]

**Question**: What's the acceptable performance degradation threshold? (e.g., < 5% increase in response time)

#### Security Testing Focus
**Security Aspects**:
- New data inputs: [Validation requirements]
- Permission changes: [New roles/permissions]
- External integrations: [Security requirements]

**Question**: Does this feature handle any sensitive data requiring additional security tests?

### 4. Test Implementation Approach

#### Incremental Testing Strategy
```yaml
# Proposed test development phases
Phase 1: Unit tests for new code (During development)
Phase 2: Integration tests (Before PR)
Phase 3: Update E2E tests (After PR approval)
Phase 4: Performance validation (Before release)
```

#### Test Prioritization Matrix
| Test Type | Priority | Effort | ROI |
|-----------|----------|--------|-----|
| Unit Tests - Core Logic | Critical | Low | High |
| Integration - API | High | Medium | High |
| E2E - Happy Path | High | High | Medium |
| E2E - Edge Cases | Medium | High | Low |
| Performance | Medium | Medium | Medium |
| Security Scan | High | Low | High |

**Question**: Does this prioritization align with your feature's risk profile?

### 5. Continuous Testing Integration

#### PR-Level Testing
**Automated Checks**:
- [ ] All new code has unit tests
- [ ] Integration tests pass
- [ ] No regression in existing tests
- [ ] Coverage doesn't decrease

**Question**: What's your current coverage, and what's the minimum acceptable for this PR?

#### Feature Flag Testing
**If using feature flags**:
- Test with flag ON
- Test with flag OFF
- Test flag transitions
- Test partial rollout scenarios

**Question**: Are you using feature flags for this release?

### 6. Test Data Requirements

#### Data Scenarios
**Required Test Data**:
- Minimum viable data: [Description]
- Edge case data: [Description]
- Performance test data: [Volume/complexity]

**Question**: Can we use existing test data, or do we need new fixtures?

### 7. Effort Estimation

#### Test Development Time
Based on the feature complexity:
- Unit tests: [X hours]
- Integration tests: [Y hours]
- E2E test updates: [Z hours]
- Total: [Sum hours]

**Question**: Is this within your sprint capacity?

### 8. Definition of Done - Testing Criteria

**Testing Checklist**:
- [ ] Unit test coverage â‰¥ ___%
- [ ] All integration tests passing
- [ ] E2E tests updated and passing
- [ ] Performance benchmarks met
- [ ] Security scan completed
- [ ] No increase in test flakiness
- [ ] Test documentation updated

**Question**: Any additional criteria specific to your team's DoD?

## Test Strategy Structure

### ðŸ“‹ Acceptance Criteria Mapping

Each acceptance criterion from the feature brief MUST have corresponding tests:

```markdown
# Test Plan: [Feature Name]

## ðŸŽ¯ Test Coverage Mapping

### Acceptance Criterion 1: [Description]
**Test Scenarios:**
- âœ… **Happy Path:** [Test description]
- âŒ **Error Case:** [Test description]
- ðŸ”„ **Edge Case:** [Test description]

### Acceptance Criterion 2: [Description]
**Test Scenarios:**
- âœ… **Integration:** [How it works with existing features]
- ðŸ”’ **Security:** [Permission/access tests]
- âš¡ **Performance:** [Load/speed tests]

## ðŸ§ª Test Strategy by Type

### Unit Tests (Coverage Target: [X]%)
- **New Components:** [List of components needing unit tests]
- **Modified Components:** [Existing code requiring updated tests]
- **[OPTIMIZE] Reusable Patterns:** [Reference to similar test suites]

### Integration Tests
- **API Endpoints:** [New/modified endpoints to test]
- **Database Operations:** [Data integrity tests]
- **Service Interactions:** [Inter-service communication tests]
- **[FLEX] Complexity-Based:** Add contract tests for complex integrations

### E2E Tests
- **User Journey 1:** [Primary feature workflow]
- **User Journey 2:** [Alternative workflow]
- **[ADAPT] Risk-Based:** Focus on high-risk user paths

## âš ï¸ Risk-Based Testing (For Complex Features)

### Technical Risks
| Risk | Impact | Test Strategy | Priority |
|------|--------|---------------|----------|
| [Risk 1] | High/Med/Low | [Specific tests] | P0/P1/P2 |
| [Risk 2] | High/Med/Low | [Specific tests] | P0/P1/P2 |

### Performance Risks
- **Baseline Metrics:** [Current performance]
- **Acceptable Degradation:** [Thresholds]
- **Test Approach:** [Load testing strategy]

## ðŸ”„ Test Implementation Phases

### Phase 1: Development Testing
- Unit tests written alongside code
- Local integration tests
- **[OPTIMIZE]:** Use existing test utilities

### Phase 2: Pre-Merge Testing
- Full integration test suite
- Security scanning
- Performance benchmarks

### Phase 3: Post-Merge Validation
- E2E test updates
- Regression suite execution
- Production monitoring setup

## ðŸ“Š Test Metrics & Success Criteria

### Coverage Requirements
- **Unit Test Coverage:** â‰¥[X]% for new code
- **Integration Coverage:** All API endpoints tested
- **E2E Coverage:** All acceptance criteria validated

### Quality Gates
- [ ] All tests passing in CI/CD
- [ ] No performance regression >5%
- [ ] Security scan clean
- [ ] No increase in test flakiness

## ðŸ”§ Test Infrastructure Needs

### Test Data Requirements
- **Fixtures:** [Required test data]
- **Mocks:** [External services to mock]
- **[OPTIMIZE]:** Reuse existing test factories

### Environment Setup
- **Dependencies:** [Test environment needs]
- **Configuration:** [Special test configs]
- **[ADAPT]:** Scale based on feature complexity
```

## Generated Test Plan

Based on our discussion and acceptance criteria analysis, I'll create your feature-specific test plan.

### Saving Test Strategy
<execute>
Store feature test approach:
mcp__lerian-memory__memory_create with operation="store_chunk", content="Feature test strategy for [feature-name]: [approach summary]"
</execute>

### Test Artifacts Location
- Test plan: `docs/pre-development/features/test-plan-[feature-name].md`
- Test scenarios: `docs/pre-development/features/test-scenarios-[feature-name].md`
- Test tasks: `docs/pre-development/tasks/test-tasks/[feature-name]/`

### Quick Start Testing
1. Begin with unit tests for new code (can start immediately)
2. Write integration tests for API changes
3. Update E2E tests for new user journeys
4. Run regression suite before merge

**Final Question**: Would you like me to generate specific test cases for the highest-risk areas of your feature?

---

*Remember: Feature testing is about balancing thoroughness with velocity. Focus on the highest-risk areas and automate repetitively.*