---
description: 
globs: 
alwaysApply: false
---
# Rule: Generating Tasks from PRD and TRD Analysis

## 🧠 Enhanced Analysis Tools - USE THESE!

**CRITICAL:** Leverage these tools throughout the task generation process:

### 🧩 Memory MCP Integration
- **Retrieve context:** `memory_search` for PRD and TRD decisions, patterns, and constraints
- **Store task logic:** `memory_store_decision` for task breakdown rationale and dependencies
- **Reference implementations:** `memory_search` for similar task structures from past projects
- **Track progress:** `memory_tasks` for task generation workflow and user feedback
- **Tags to use:** `["tasks", "breakdown", "atomic", "feature-name", "implementation"]`

### 🔄 Sequential Thinking MCP
- **Use for:** Complex dependency analysis, task prioritization, atomic validation
- **Pattern:** Requirements → technical components → task boundaries → dependency mapping
- **Benefit:** Ensures tasks are truly atomic and deliver working software increments
- **When:** During task boundary definition and dependency analysis

### 🤖 Zen MCP Integration (HIGHLY RECOMMENDED)
**Why Use Zen MCP:**
- **Task Analysis**: Use `mcp__zen__thinkdeep` for breaking down complex features into atomic tasks
- **Dependency Mapping**: Use `mcp__zen__analyze` to identify task dependencies and sequencing
- **Trade-off Evaluation**: Use `mcp__zen__chat` to explore different task organization approaches
- **Quality Validation**: Use `mcp__zen__codereview` to validate task completeness

**Example Usage:**
```
mcp__zen__thinkdeep
  prompt="Breaking down [feature] into atomic, deployable tasks with clear dependencies"
  model="pro"
  thinking_mode="high"
  focus_areas=["task-breakdown", "dependencies", "atomic-validation"]
```

### 🔍 Task Tool Usage (USE LIBERALLY)
**When to Use Task():**
- Finding similar task breakdowns in existing features
- Searching for atomic task patterns in the codebase
- Discovering how similar features were decomposed
- Parallel analysis of task dependencies

**Example:**
```
Task
  description="Find atomic task patterns"
  prompt="Search for existing task breakdowns, feature decompositions, and atomic implementation patterns that deliver working increments"
```

**Pro tip:** Store task breakdown decisions to improve future task generation accuracy!

## Goal

To autonomously generate atomic, functional tasks based on the strategic technical decision made in the TRD phase. This prompt operates with high autonomy, generating comprehensive task breakdowns with confidence scoring and selective user interaction only for low-confidence areas.

## Process - Autonomous Task Generation

### Phase 1: Context Gathering & Analysis

1. **Parallel Document Processing:**
   ```yaml
   Simultaneous Actions:
     - Read PRD with confidence scores
     - Read TRD with selected approach
     - Search Memory for task patterns
     - Analyze technical dependencies
   ```
   
   🧩 **Memory Actions:**
   ```
   # Find similar task breakdowns
   mcp__lerian-memory__memory_read
     operation="search"
     options={
       "query": "task breakdown [feature-type] atomic phases",
       "repository": "github.com/[org]/[repo]"
     }
   ```

2. **Confidence-Based Task Generation:**
   - For high-confidence components: Generate complete task structure
   - For medium-confidence: Generate with alternatives noted
   - For low-confidence: Flag for selective review
   
   🔄 **Sequential Thinking:**
   ```
   mcp__sequential-thinking__sequentialthinking
     thought="Breaking down [feature] into atomic phases: 1) Foundation, 2) Core functionality, 3) Integration, 4) Polish"
     nextThoughtNeeded=true
     thoughtNumber=1
     totalThoughts=5
   ```

3. **Intelligent Task Structuring:**
   ```yaml
   Task Generation Rules:
     - Each task: 4-8 hours for senior developer
     - Must deliver working increment
     - Include verification criteria
     - Map to specific PRD/TRD sections
     - Calculate confidence per task
   ```

### Phase 2: Selective Interaction (Only if needed)

**High Autonomy Mode (Default):**
- If overall confidence > 80%: Generate all tasks autonomously
- Present summary with task count and timeline
- No user interaction required

**Selective Review Mode (Confidence 50-80%):**
```markdown
## Task Breakdown Generated

### 📊 Confidence Summary
- High Confidence Tasks: 12 (75%)
- Medium Confidence: 3 (19%)  
- Low Confidence: 1 (6%)

### 🟡 Medium Confidence Tasks - Quick Review:
**Task 5: Data Migration Pipeline**
- Option A: Batch processing with checkpoints
- Option B: Stream processing with replay
- **Recommended**: Option A (proven in similar migrations)

### 🔴 Low Confidence Task - Your Input Needed:
**Task 8: Legacy System Integration**
- No similar patterns found
- Need clarification on: API format, authentication method
```

**Low Confidence Mode (< 50%):**
- Request strategic guidance on task organization
- Present multiple complete approaches
- Wait for user selection

## Input Documents Analysis

### PRD Analysis Requirements
- **Development Roadmap (Section 13):** Extract the high-level phases and their scope
- **Logical Dependency Chain (Section 14):** Understand the foundation-first approach
- **Functional Requirements (Section 5):** Map requirements to deliverable phases
- **User Stories (Section 3):** Identify user value delivery points
- **Success Metrics (Section 9):** Define measurable outcomes for each task

### TRD Analysis Requirements
- **Implementation Roadmap (Section 16):** Extract technical phases and dependencies
- **Component Design (Section 6):** Understand technical component breakdown
- **Technology Stack (Section 3):** Identify technical deliverables
- **Testing Strategy (Section 12):** Ensure each phase includes testing
- **Deployment Architecture (Section 13):** Define deployment milestones

## Atomic Phase Principles

### Definition of "Atomic"
Each task MUST be:
- **Self-Contained:** Can be developed, tested, and deployed independently
- **Functional:** Delivers working software that users can interact with
- **Testable:** Has clear acceptance criteria and can be validated
- **Deployable:** Results in a runnable application (even if limited scope)
- **Valuable:** Provides measurable user or business value

### Phase Validation Checklist
Before finalizing each task, ensure:
- [ ] **Independent Deployment:** Can be deployed without dependencies on future phases
- [ ] **User Interaction:** Users can perform meaningful actions with this phase alone
- [ ] **Complete Feature Set:** All components needed for the phase are included
- [ ] **Quality Standards:** Testing, monitoring, and observability are included
- [ ] **Rollback Capability:** Can be safely rolled back if issues arise

## Confidence Scoring for Tasks

### Task Confidence Calculation
```yaml
Task Confidence Components:
  pattern_match: [0-40]
    - Exact task pattern: 40
    - Similar implementation: 25-35
    - Partial match: 10-20
    - No match: 0
    
  technical_clarity: [0-30]
    - Clear implementation path: 30
    - Minor uncertainties: 20
    - Significant unknowns: 10
    - Experimental approach: 0
    
  dependency_risk: [0-30]
    - No external dependencies: 30
    - Known stable dependencies: 20
    - Some uncertainty: 10
    - High dependency risk: 0
    
  total_confidence: [sum]
  threshold:
    - 80+: Proceed autonomously
    - 50-79: Note alternatives
    - <50: Request input
```

## User Interaction Model - Efficiency First

### Autonomous Generation (80%+ Confidence)
```markdown
## ✅ Task Breakdown Complete

Generated 15 atomic tasks with high confidence based on:
- Similar pattern from auth-service project (95% match)
- Standard implementation approach
- Proven task sizing from 3 previous projects

**Summary:**
- Total Tasks: 15
- Estimated Duration: 3 sprints
- First Deliverable: Day 3
- MVP Complete: Sprint 1

**Proceeding with task generation...**
```

### Selective Review (50-79% Confidence)
Only present tasks needing input, not entire list

### Strategic Guidance (<50% Confidence)
Present high-level approaches for selection
```
AI: "Based on the PRD and TRD, I've created the following atomic task breakdown:

**MT-001: Foundation Phase** (2 weeks)
- Basic application structure with authentication
- Delivers: Working login system

**MT-002: Core Feature Phase** (3 weeks)  
- Implement primary business logic
- Delivers: Main user workflow

**MT-003: Enhancement Phase** (2 weeks)
- Add advanced features
- Delivers: Complete feature set

Would you like to adjust the prioritization or scope of any tasks?"

[WAIT FOR USER RESPONSE]

User: "Can we split MT-002 into two smaller tasks?"

AI: "Certainly! Let me restructure MT-002 into two atomic tasks..."
```

## Task Structure - Confidence-Optimized Format

### High-Confidence Task Template (80%+ confidence)
```markdown
## MT-XXX: [Task Name] 🟢 [95%]

**Pattern Source:** [project-name::component] (95% match)
**Duration:** [X] days (high certainty)
**Delivers:** [Specific working feature]

### Implementation
- Backend: [Components based on pattern]
- Frontend: [UI elements if applicable]
- Data: [Schema/migrations needed]

### Verification
- [ ] [Specific user action possible]
- [ ] [Measurable outcome achieved]
- [ ] Tests pass, quality checks complete

**Dependencies:** [MT-XXX] or None
**Git Branch:** `feature/MT-XXX-[description]`
```

### Medium-Confidence Task Template (50-79%)
```markdown
## MT-XXX: [Task Name] 🟡 [65%]

**Approach Options:**
- Selected: [Approach A - reason]
- Alternative: [Approach B - trade-offs]

**Duration:** [X] days (±30% variance)
**Delivers:** [Intended outcome]

[Rest follows high-confidence structure]

**Review Notes:** [Specific areas that may need adjustment]
```

### Low-Confidence Task Template (<50%)
```markdown
## MT-XXX: [Task Name] 🔴 [30%]

**Status:** Needs clarification
**Blocking Questions:**
1. [Specific unknown that prevents planning]
2. [Technical decision needed]

**Proposed Approaches:**
- Option 1: [Description with pros/cons]
- Option 2: [Description with pros/cons]

**Duration:** TBD pending decisions
```

### Atomic Validation Criteria
- **Task ID:** MT-[number] (e.g., MT-001, MT-002)
- **Atomic Test:** Can be deployed independently
- **Value Test:** Delivers user-facing functionality
- **Duration Target:** 4-8 hours senior developer time

### 🔀 Git Workflow for Implementation
**CRITICAL:** Each task will be broken into sub-tasks. Each sub-task MUST:
1. **Create branch:** `git checkout -b feature/ST-MT-[task-id]-[num]-[desc]`
2. **Implement** the sub-task requirements
3. **Commit with details:** Include implementation notes and any deviations
4. **Push and PR:** Create pull request for code review before merging

### 2. Deliverable Description
- **Primary Deliverable:** What working software will be delivered
- **User Value:** How users benefit from this phase
- **Business Value:** What business objective this achieves
- **Working Definition:** Exact criteria for "working application"

### 3. Functional Scope
- **Included Features:** Specific features implemented in this phase
- **User Capabilities:** What users can accomplish with this phase
- **API Endpoints:** Which endpoints are functional (if applicable)
- **UI Components:** Which interface elements are working (if applicable)

### 4. Technical Scope
- **Architecture Components:** Which technical components are built
- **Data Model:** What data structures are implemented
- **Integration Points:** Which integrations are functional
- **Infrastructure:** What deployment infrastructure is required
- **File Organization:** Target <300 lines per file, max 500 lines (split logically if larger)
- **Architecture Patterns:** Hexagonal Architecture compliance requirements
- **lib-commons Integration:** Required lib-commons components and patterns
- **lib-auth Integration:** Authentication and authorization patterns (for Midaz projects)

### 5. Dependencies
- **Required Previous Phases:** Which tasks must be completed first
- **External Dependencies:** Third-party services, libraries, or tools needed
- **Internal Dependencies:** Existing system components required
- **Blocking Dependencies:** What could prevent this phase from starting

### 6. Acceptance Criteria
- **Functional Criteria:** Numbered list of functional requirements
- **Technical Criteria:** Numbered list of technical requirements
- **Quality Criteria:** Performance, security, and reliability standards
- **User Acceptance:** How users will validate the deliverable

### 7. Testing Requirements
- **Unit Testing:** Required unit test coverage and scope
- **Integration Testing:** Integration points that must be tested
- **Cross-Task Integration:** How this task integrates with previous completed tasks
- **End-to-End Testing:** Complete user workflows to validate
- **Performance Testing:** Load and performance validation requirements
- **System Integration:** Full system validation with all completed tasks

### 8. Deployment Definition
- **Deployment Target:** Where this phase will be deployed
- **Configuration:** Required environment configuration
- **Data Migration:** Any data migration requirements
- **Rollback Plan:** How to safely revert this deployment

### 9. Success Metrics
- **Technical Metrics:** Performance, uptime, error rates
- **User Metrics:** Usage, adoption, satisfaction indicators
- **Business Metrics:** Revenue, efficiency, cost impact
- **Quality Metrics:** Code coverage, bug rates, security compliance

### 10. Risk Assessment
- **Technical Risks:** Implementation challenges and mitigations
- **Integration Risks:** Potential integration issues
- **Timeline Risks:** Factors that could affect delivery
- **Quality Risks:** Potential quality or performance issues

### 11. Quality Assurance Requirements
**CRITICAL:** Each task and sub-task MUST include programming language-specific quality checks:

#### Go Projects
```bash
# Run after each task/sub-task completion
make fmt              # Code formatting
make vet              # Static analysis
gosec ./...          # Security analysis
govulncheck ./...    # Vulnerability check
perfsprint ./...     # Performance analysis
make lint            # Comprehensive linting
go test ./...        # Test execution

# After quality checks pass, build to ensure compilation
go build ./...       # Build verification
```

#### TypeScript/JavaScript Projects
```bash
# Run after each task/sub-task completion
npm run lint         # ESLint code quality
npm run typecheck    # TypeScript validation
npm test            # Test execution
npm run build       # Production build verification
```

#### Python Projects
```bash
# Run after each task/sub-task completion
ruff check .         # Linting and formatting
mypy .              # Type checking
pytest              # Test execution
black .             # Code formatting
```

#### Other Languages
- **Rust:** `cargo fmt && cargo clippy && cargo test`
- **Java:** `mvn compile && mvn test && mvn checkstyle:check`
- **C#:** `dotnet format && dotnet test && dotnet build`

**FAILURE PROTOCOL:** If quality checks fail:
1. Fix issues immediately before proceeding
2. Document quality violations in memory: `memory_store_chunk` with tag `quality-violation`
3. Re-run checks to ensure resolution
4. Only mark task as complete when ALL quality checks pass

### 🚨 Pre-Emptive Quality Guidance (CRITICAL)

**IMPORTANT:** Use these patterns to avoid common quality check failures from the start:

#### Go Language Anti-Patterns to Avoid
**Error Handling:**
```go
// ❌ WRONG - causes errcheck/staticcheck violations
fmt.Errorf("user not found")  // For static messages

// ✅ CORRECT - use errors.New for static messages
errors.New("user not found")

// ✅ CORRECT - use fmt.Errorf only with variables
fmt.Errorf("user %s not found", userID)
```

**String Operations:**
```go
// ❌ WRONG - causes perfsprint violations
fmt.Sprintf("%d", number)     // Use strconv.Itoa(number)
fmt.Sprintf("%s", variable)   // Use string(variable) or direct concatenation
fmt.Sprintf("prefix%s", var)  // Use "prefix"+var

// ✅ CORRECT - performance optimized
strconv.Itoa(number)          // For integer to string
"prefix" + variable           // For simple concatenation
```

**Function Complexity (Prevent gocyclo/nestif violations):**
```go
// ❌ WRONG - high complexity, deep nesting
func ProcessTask(task Task) error {
    if task.Valid {
        if task.Type == "important" {
            if task.Priority > 5 {
                if task.Assignee != "" {
                    // Deep nesting causes nestif violations
                    // Complex logic causes gocyclo violations
                }
            }
        }
    }
    return nil
}

// ✅ CORRECT - early returns, guard clauses
func ProcessTask(task Task) error {
    if !task.Valid {
        return errors.New("invalid task")
    }
    if task.Type != "important" {
        return nil  // Early return reduces complexity
    }
    if task.Priority <= 5 {
        return nil  // Guard clause prevents nesting
    }
    if task.Assignee == "" {
        return errors.New("task requires assignee")
    }
    
    // Main logic here - minimal nesting
    return task.Process()
}
```

**Preallocation (Prevent performance issues):**
```go
// ❌ WRONG - causes prealloc violations
var items []Item
for i := 0; i < count; i++ {
    items = append(items, createItem(i))
}

// ✅ CORRECT - pre-allocate slices
items := make([]Item, 0, count)  // Pre-allocate capacity
for i := 0; i < count; i++ {
    items = append(items, createItem(i))
}
```

**Unused Code Prevention:**
```go
// ❌ WRONG - causes unparam/unused violations
func ProcessData(ctx context.Context, data []byte, unused string) error {
    // ctx never used - unparam violation
    // unused parameter - unparam violation
    return processBytes(data)
}

// ✅ CORRECT - use all parameters or remove them
func ProcessData(data []byte) error {
    return processBytes(data)
}

// ✅ OR use context explicitly
func ProcessData(ctx context.Context, data []byte) error {
    select {
    case <-ctx.Done():
        return ctx.Err()
    default:
        return processBytes(data)
    }
}
```

#### TypeScript/JavaScript Anti-Patterns
```typescript
// ❌ WRONG - common linting violations
const data: any = fetchData();              // Avoid 'any' type
let variable = "test";                      // Use const when possible
if (condition == true)                      // Use === instead of ==

// ✅ CORRECT - lint-compliant patterns
const data: UserData = await fetchData();  // Explicit typing
const variable = "test";                    // const for immutable
if (condition === true)                     // Strict equality
```

#### Python Anti-Patterns
```python
# ❌ WRONG - common ruff/mypy violations
def process_data(data):                     # Missing type hints
    return data.get("value")                # Unsafe dict access

# ✅ CORRECT - type-safe patterns
def process_data(data: Dict[str, Any]) -> Optional[str]:
    return data.get("value")
```

### 🎯 Complexity Management Strategies

**Keep Functions Small:**
- **Target:** <20 lines per function
- **Maximum:** 50 lines (split if larger)
- **Strategy:** Extract helper functions for complex logic

**Reduce Cognitive Complexity:**
- **Use early returns** instead of nested if statements
- **Extract validation logic** into separate functions
- **Use guard clauses** to handle edge cases first
- **Limit nesting depth** to maximum 3 levels

**Error Handling Patterns:**
```go
// ✅ RECOMMENDED pattern for Go error handling
func ProcessWithValidation(input Input) (*Result, error) {
    if err := input.Validate(); err != nil {
        return nil, fmt.Errorf("validation failed: %w", err)
    }
    
    result, err := input.Process()
    if err != nil {
        return nil, fmt.Errorf("processing failed: %w", err)
    }
    
    return result, nil
}
```

### 📏 File Size Guidelines for Quality

**Optimal File Sizes:**
- **Go files:** <300 lines target, 500 lines maximum
- **TypeScript:** <250 lines target, 400 lines maximum  
- **Python:** <200 lines target, 350 lines maximum

**When approaching limits:**
1. Extract separate modules/packages
2. Split into logical components
3. Create helper/utility files
4. Use composition over inheritance

### 🔍 Pre-Implementation Quality Checklist

Before writing any code, ensure tasks specify:
- [ ] **Error handling strategy** - Use proper error patterns
- [ ] **Function complexity limits** - Maximum 3 levels of nesting
- [ ] **Parameter usage** - All parameters must be used or explicitly ignored
- [ ] **String operation efficiency** - Use appropriate string functions
- [ ] **Slice preallocation** - Pre-allocate when size is known
- [ ] **Type safety** - Explicit types in TypeScript, type hints in Python
- [ ] **Security considerations** - Input validation, output sanitization

## Example Task Format

```markdown
## MT-001: CLI Foundation with Local Storage

### 1. Task Overview
- **Task ID:** MT-001
- **Task Name:** CLI Foundation with Local Storage
- **Phase Type:** Foundation
- **Duration Estimate:** 4-6 weeks
- **Atomic Validation:** ✅ Delivers a working CLI that can manage tasks locally

### 2. Deliverable Description
- **Primary Deliverable:** Functional CLI application that manages tasks with local file storage
- **User Value:** Developers can create, list, update, and complete tasks locally
- **Business Value:** Provides immediate task management capability without server dependency
- **Working Definition:** CLI responds to all basic commands and persists data locally

### 3. Functional Scope
- **Included Features:**
  - Task creation (`lmmc add "task description"`)
  - Task listing (`lmmc list`)
  - Task status updates (`lmmc start`, `lmmc done`)
  - Local configuration management
  - Shell completion installation
- **User Capabilities:**
  - Create new tasks with descriptions
  - View all tasks with status filtering
  - Mark tasks as started or completed
  - Configure CLI settings
- **API Endpoints:** N/A (local-only phase)
- **UI Components:** Terminal CLI interface with colored output

[Continue with remaining sections...]
```

## Quality Standards

### Completeness Validation
Each task must include:
- [ ] **Clear Success Definition:** Unambiguous criteria for completion
- [ ] **User Workflow:** Complete user journey from start to finish
- [ ] **Technical Architecture:** All required components identified
- [ ] **Testing Strategy:** Comprehensive testing plan
- [ ] **Deployment Plan:** Clear deployment and validation process

### Atomic Validation
Verify each task:
- [ ] **Standalone Value:** Provides value without future phases
- [ ] **Complete Feature Set:** All components needed are included
- [ ] **Working Software:** Results in functional, testable application
- [ ] **User-Facing:** Users can interact with and benefit from the deliverable
- [ ] **Production-Ready:** Includes monitoring, logging, and observability
- [ ] **Deployable:** Complete deployment process defined

## Task Deployment Workflow

### Git Workflow for Tasks (Atomic Phases):

1. **Task Branch:** Create tasks-implementation branch for each task
   ```bash
   git checkout -b tasks-implementation/T-[task-id]-[short-name]
   # Example: git checkout -b tasks-implementation/T-001-cli-foundation
   ```

2. **Sub-Task Integration:** All sub-task PRs merge into tasks-implementation branch

3. **Task Completion:** When all sub-tasks are complete:
   ```bash
   # Verify working deployment
   make build && make test && make deploy-staging
   
   # Create PR to develop
   git push origin tasks-implementation/T-[task-id]-[short-name]
   # Create PR: tasks-implementation/T-[task-id] → develop
   ```

4. **Task PR Requirements:**
   - Complete working application increment
   - All tests passing
   - Deployment verified
   - Documentation updated
   - Architecture compliance verified
   - Integration testing with previous tasks completed

## Output Structure

### File Organization
```
docs/pre-development/tasks/
├── tasks-[feature-name].md     # This document
├── phase-dependencies.md            # Dependency visualization
└── acceptance-checklist.md          # Validation checklist
```

### Dependencies Visualization
Include a mermaid diagram showing phase dependencies:
```mermaid
graph TD
    A[MT-001: CLI Foundation] --> B[MT-002: AI Integration]
    B --> C[MT-003: Server Integration]
    C --> D[MT-004: Advanced Features]
    D --> E[MT-005: Production Deployment]
```

## Cross-Task Integration Strategy

### Integration Testing Approach

**After Each Task Completion:**
1. **Backward Compatibility:** Ensure new task doesn't break previous tasks
2. **API Compatibility:** Verify interface contracts remain stable
3. **Data Flow Validation:** Test data flow through completed system
4. **Performance Impact:** Measure system performance with new task
5. **User Journey Testing:** Validate complete user workflows

**Integration Test Types:**
- **API Integration:** Test endpoints work together
- **Data Integration:** Verify data consistency across tasks
- **UI Integration:** Ensure seamless user experience
- **Performance Integration:** System-wide performance validation
- **Security Integration:** End-to-end security validation

**Integration Environments:**
- **Integration Environment:** Dedicated environment for cross-task testing
- **Staging Environment:** Production-like environment for final validation
- **Performance Environment:** Dedicated environment for load testing

### Integration Documentation

**Required for Each Task:**
- **Integration Points:** How this task connects to previous tasks
- **Interface Contracts:** APIs, data formats, communication protocols
- **Integration Tests:** Specific tests to validate cross-task functionality
- **Rollback Procedures:** How to safely revert if integration fails
- **Performance Benchmarks:** Expected system performance with this task

## Integration with Development Chain

### Input Sources
- **PRD Section 13:** Development Roadmap phases
- **PRD Section 14:** Logical Dependency Chain
- **TRD Section 16:** Implementation Roadmap
- **User Stories:** Value delivery validation

### Output Usage
- **generate-sub-tasks.mdc:** Uses tasks to create detailed sub-tasks
- **Project Planning:** Provides sprint/milestone boundaries
- **Team Coordination:** Defines handoff points between teams
- **Stakeholder Communication:** Shows delivery milestones
- **Integration Testing:** Provides framework for cross-task integration

## Validation Checklist

Before finalizing tasks:
- [ ] **PRD Alignment:** All PRD phases represented in tasks
- [ ] **TRD Alignment:** Technical roadmap matches task structure
- [ ] **Atomic Principle:** Each task delivers working software
- [ ] **Dependency Logic:** Dependencies are logical and minimal
- [ ] **User Value:** Each phase provides measurable user benefit
- [ ] **Technical Completeness:** All required components included
- [ ] **Testing Coverage:** Comprehensive testing strategy defined
- [ ] **Deployment Readiness:** Clear deployment and validation plan

## Final Instructions

1. **Read Both Documents:** Always analyze both PRD and TRD thoroughly
2. **Apply Atomic Thinking:** Each task must be a complete, working increment
3. **Validate Dependencies:** Ensure dependencies are necessary and minimal
4. **Define Success Clearly:** Each task must have unambiguous completion criteria
5. **Include Quality Measures:** Testing, monitoring, and observability in every phase
6. **Plan for Users:** Each phase should provide tangible user value
7. **Next Step:** After completing tasks, use `@generate-sub-tasks.mdc` for detailed breakdown

## Relationship to Chain

This rule is the third step in the development chain:
1. **create-prd.mdc** → Defines business requirements and high-level phases
2. **create-trd.mdc** → Translates to technical specifications and implementation roadmap
3. **generate-tasks.mdc** → Creates atomic, functional development phases ← **YOU ARE HERE**
4. **generate-sub-tasks.mdc** → Breaks down each task into implementable sub-tasks